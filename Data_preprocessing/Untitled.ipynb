{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d879692-ad11-4790-b0ee-dd91cbf5e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu126\n",
      "CUDA version: 12.6\n",
      "cuDNN version: 91002\n",
      "Is CUDA available?: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775c125d-ff78-4d97-a8a9-8b683e690619",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch6detail10class_baseC2ERKSsS3_SsRKSt9type_infoS6_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AG_NEWS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n",
      "File \u001b[0;32m~/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vikhil/lib/python3.10/site-packages/torch/_ops.py:1478\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1473\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1478\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/vikhil/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch6detail10class_baseC2ERKSsS3_SsRKSt9type_infoS6_"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 1. Load data (iterator yielding (label, text))\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "# 2. Tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# 3. Build vocabulary from training data (CPU work)\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 4. Function to preprocess one example\n",
    "def text_to_tensor(text: str):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = [vocab[token] for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "# 5. Example: prepare a batch\n",
    "texts = [\"hello world\", \"this is a test\"]\n",
    "tensors = [text_to_tensor(t) for t in texts]\n",
    "\n",
    "# Pad to same length (simple example)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "batch = pad_sequence(tensors, batch_first=True, padding_value=vocab[\"<unk>\"])  # shape: (batch_size, max_len)\n",
    "\n",
    "labels = torch.tensor([0, 1], dtype=torch.long)  # dummy labels\n",
    "\n",
    "# 6. Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch = batch.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# 7. Define a simple model\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        # embed: (batch, seq_len, embed_dim)\n",
    "        emb = self.embedding(x)\n",
    "        # simple: average pooling over sequence\n",
    "        pooled = emb.mean(dim=1)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "model = SimpleModel(len(vocab), embed_dim=32, num_class=4).to(device)\n",
    "\n",
    "# 8. Forward pass\n",
    "output = model(batch)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc7ec30-fd28-4d95-aa0f-8b902dd0abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.8.0+cu126\n",
      "Uninstalling torch-2.8.0+cu126:\n",
      "  Successfully uninstalled torch-2.8.0+cu126\n",
      "Found existing installation: torchvision 0.23.0+cu126\n",
      "Uninstalling torchvision-0.23.0+cu126:\n",
      "  Successfully uninstalled torchvision-0.23.0+cu126\n",
      "Found existing installation: torchtext 0.18.0\n",
      "Uninstalling torchtext-0.18.0:\n",
      "  Successfully uninstalled torchtext-0.18.0\n"
     ]
    }
   ],
   "source": [
    "# Deactivate if you need to, then reactivate to be sure\n",
    "# conda deactivate\n",
    "# conda activate vikhil\n",
    "\n",
    "!pip uninstall torch torchvision torchtext -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00aa898-2aa1-46c8-9089-6c7c12cc0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "# --- Download NLTK data (if needed) ---\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration & Global Functions\n",
    "# ==============================================================================\n",
    "FILE_PATH = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/fake_job_cleaned_dataset.csv'\n",
    "MODEL_SAVE_PATH = 'hybrid_model.pth'\n",
    "TOKENIZER_SAVE_PATH = 'tokenizer.pickle'\n",
    "COLS_SAVE_PATH = 'train_cols.pkl'\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 512\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# --- Reusable Functions ---\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "def create_features(df):\n",
    "    df['text'] = (\n",
    "        df['title'].fillna('') + ' ' + df['location'].fillna('') + ' ' +\n",
    "        df['department'].fillna('') + ' ' + df['company_profile'].fillna('') + ' ' +\n",
    "        df['description'].fillna('') + ' ' + df['requirements'].fillna('') + ' ' +\n",
    "        df['benefits'].fillna('') + ' ' + df['employment_type'].fillna('') + ' ' +\n",
    "        df['required_experience'].fillna('') + ' ' + df['required_education'].fillna('') + ' ' +\n",
    "        df['industry'].fillna('') + ' ' + df['function'].fillna('')\n",
    "    )\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    for col in ['telecommuting', 'has_company_logo', 'has_questions']:\n",
    "        df[col] = df[col].astype(float)\n",
    "    categorical_cols = ['employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
    "    return pd.get_dummies(df, columns=categorical_cols, dummy_na=True, drop_first=True)\n",
    "\n",
    "class JobDataset(Dataset):\n",
    "    # ... (code remains the same as before)\n",
    "    def __init__(self, texts, tabular, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.tabular = tabular\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        tabular_data = self.tabular[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = self.tokenizer.texts_to_sequences([text])[0]\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(tabular_data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # ... (code remains the same as before)\n",
    "    texts, tabular, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    return texts_padded, torch.stack(tabular), torch.stack(labels)\n",
    "    \n",
    "class HybridRNNModel(nn.Module):\n",
    "    # ... (code remains the same as before)\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, tabular_feature_count):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.tabular_fc = nn.Linear(tabular_feature_count, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        lstm_output_size = hidden_dim * 2\n",
    "        self.fc_combined = nn.Linear(lstm_output_size + 32, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, text, tabular_features):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        tabular_out = self.relu(self.tabular_fc(tabular_features))\n",
    "        combined = torch.cat((hidden, tabular_out), dim=1)\n",
    "        return self.fc_combined(combined)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Prediction Pipeline Functions\n",
    "# ==============================================================================\n",
    "def extract_structured_data(raw_text):\n",
    "    # ... (code remains the same as before)\n",
    "    data = { 'title': '', 'location': '', 'department': '', 'company_profile': '', 'description': '', 'requirements': '', 'benefits': '', 'employment_type': 'Unspecified', 'required_experience': 'Unspecified', 'required_education': 'Unspecified', 'industry': 'Unspecified', 'function': 'Unspecified', 'telecommuting': 0.0, 'has_company_logo': 1.0, 'has_questions': 0.0 }\n",
    "    lines = raw_text.strip().split('\\\\n')\n",
    "    data['title'] = lines[0].strip() if lines else ''\n",
    "    text_lower = raw_text.lower()\n",
    "    desc_match = re.search(r'(job description|responsibilities)(.*)(qualifications|requirements)', text_lower, re.S | re.I)\n",
    "    if desc_match: data['description'] = desc_match.group(2).strip()\n",
    "    req_match = re.search(r'(qualifications|requirements)(.*)(benefits|why join)', text_lower, re.S | re.I)\n",
    "    if req_match: data['requirements'] = req_match.group(2).strip()\n",
    "    ben_match = re.search(r'(benefits|why join)(.*)', text_lower, re.S | re.I)\n",
    "    if ben_match: data['benefits'] = ben_match.group(2).strip()\n",
    "    if 'full time' in text_lower: data['employment_type'] = 'Full-time'\n",
    "    elif 'contract' in text_lower: data['employment_type'] = 'Contract'\n",
    "    if not data['description']: data['description'] = raw_text\n",
    "    return data\n",
    "\n",
    "def predict_job_posting(input_data, model, tokenizer, train_df_cols, device):\n",
    "    # ... (code remains the same as before, but with added comments)\n",
    "    model.eval()\n",
    "    if isinstance(input_data, str):\n",
    "        print(\"Raw text detected. Parsing into structured format...\")\n",
    "        structured_data = extract_structured_data(input_data)\n",
    "        df_pred = pd.DataFrame([structured_data])\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be a raw string.\")\n",
    "\n",
    "    df_pred_processed = create_features(df_pred)\n",
    "    missing_cols = set(train_df_cols) - set(df_pred_processed.columns)\n",
    "    for c in missing_cols:\n",
    "        df_pred_processed[c] = 0\n",
    "    df_pred_processed = df_pred_processed[train_df_cols]\n",
    "\n",
    "    text_to_predict = df_pred_processed['text']\n",
    "    tabular_cols = [c for c in train_df_cols if c not in ['text', 'fraudulent']]\n",
    "    tabular_to_predict = df_pred_processed[tabular_cols].values\n",
    "\n",
    "    tokenized = tokenizer.texts_to_sequences(text_to_predict)\n",
    "    text_tensor = pad_sequence([torch.tensor(t) for t in tokenized], batch_first=True, padding_value=0, total_length=MAX_LEN).to(device)\n",
    "    tabular_tensor = torch.tensor(tabular_to_predict, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(text_tensor, tabular_tensor)\n",
    "        probability = torch.sigmoid(prediction).item()\n",
    "\n",
    "    result = \"Fake Job\" if probability > 0.5 else \"Real Job\"\n",
    "    print(f\"\\\\n---> Prediction: {result} (Probability of being fake: {probability:.4f})\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Main Execution Block\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    # --- PHASE 1: TRAINING ---\n",
    "    print(\"--- Starting Training Phase ---\")\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    df_processed = create_features(df)\n",
    "    \n",
    "    y = df_processed['fraudulent'].values\n",
    "    X_text = df_processed['text']\n",
    "    \n",
    "    train_df_columns = [col for col in df_processed.columns if col != 'fraudulent']\n",
    "    tabular_cols_train = [c for c in train_df_columns if c != 'text']\n",
    "    X_tabular = df_processed[tabular_cols_train].values\n",
    "\n",
    "    X_text_train, _, X_tabular_train, _, y_train, _ = train_test_split(\n",
    "        X_text, X_tabular, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_text_train)\n",
    "    \n",
    "    train_dataset = JobDataset(X_text_train, X_tabular_train, pd.Series(y_train), tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = HybridRNNModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS,\n",
    "                         BIDIRECTIONAL, DROPOUT, tabular_feature_count=X_tabular.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    \n",
    "    print(f\"\\nStarting Model Training on {device}...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for text, tab, labels in train_loader:\n",
    "            text, tab, labels = text.to(device), tab.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text, tab).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1:02} | Training Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    print(\"\\\\n--- Training Complete ---\")\n",
    "    \n",
    "    # --- Save the trained model and tokenizer ---\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    with open(TOKENIZER_SAVE_PATH, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(COLS_SAVE_PATH, 'wb') as handle:\n",
    "        pickle.dump(train_df_columns, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"Tokenizer saved to: {TOKENIZER_SAVE_PATH}\")\n",
    "    print(f\"Training columns saved to: {COLS_SAVE_PATH}\")\n",
    "\n",
    "\n",
    "    # --- PHASE 2: INFERENCE (PREDICTION) ---\n",
    "    print(\"\\n\\n--- Starting Inference Phase ---\")\n",
    "    # Load the assets we just saved\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    with open(TOKENIZER_SAVE_PATH, 'rb') as handle:\n",
    "        loaded_tokenizer = pickle.load(handle)\n",
    "        \n",
    "    with open(COLS_SAVE_PATH, 'rb') as handle:\n",
    "        loaded_cols = pickle.load(handle)\n",
    "\n",
    "    # We need the number of tabular features to initialize the model structure correctly\n",
    "    tabular_feature_count = len([c for c in loaded_cols if c != 'text'])\n",
    "\n",
    "    loaded_model = HybridRNNModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS,\n",
    "                                 BIDIRECTIONAL, DROPOUT, tabular_feature_count=tabular_feature_count).to(device)\n",
    "    \n",
    "    loaded_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    print(\"Trained model and tokenizer loaded successfully.\")\n",
    "    \n",
    "    # --- Get interactive user input ---\n",
    "    input_text = input(\"\\\\n>>> Please paste the job description here and press Enter:\\\\n\")\n",
    "    \n",
    "    # --- Make the prediction ---\n",
    "    predict_job_posting(input_text, loaded_model, loaded_tokenizer, loaded_cols, device)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        print(f\"Error: Dataset file '{FILE_PATH}' not found. Please ensure it is in the correct directory.\")\n",
    "    else:\n",
    "        main()!# This cleans out pip's local cache\n",
    "pip cache purge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
