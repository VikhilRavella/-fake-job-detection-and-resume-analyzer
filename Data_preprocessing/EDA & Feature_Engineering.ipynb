{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76ed620-a27f-4701-afad-621c274b53ee",
   "metadata": {},
   "source": [
    "# Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda576ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n",
      "TorchText version: 0.18.0+cpu\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"TorchText version:\", torchtext.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f997b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c6f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/fake_job_cleaned_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "398f4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class GPUTextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for comprehensive text preprocessing (cleaning, tokenization,\n",
    "    lemmatization) that numericalizes the text and ensures the final tensors \n",
    "    are ready on the GPU (or CPU as fallback).\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, device='cuda', perform_spell_correction=False):\n",
    "        \n",
    "        # --- 1. Device Setup ---\n",
    "        # Explicitly check for CUDA and set the device (GPU/CPU)\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # --- 2. Preprocessing Tools ---\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.perform_spell_correction = perform_spell_correction\n",
    "        \n",
    "        if self.perform_spell_correction:\n",
    "            # Note: You need 'pip install pyspellchecker' for this to work\n",
    "            try:\n",
    "                from spellchecker import SpellChecker\n",
    "                self.spell = SpellChecker()\n",
    "            except ImportError:\n",
    "                print(\"Warning: 'spellchecker' not installed. Spell correction will be skipped.\")\n",
    "                self.perform_spell_correction = False\n",
    "        \n",
    "        # --- 3. Initial Processing (on CPU) ---\n",
    "        self.cleaned_texts = [self.clean_text(t) for t in texts]\n",
    "        \n",
    "        # --- 4. Build Vocabulary ---\n",
    "        def token_iterator():\n",
    "            for text in self.cleaned_texts:\n",
    "                # Need to check if text is not empty after cleaning\n",
    "                if text:\n",
    "                    yield self.tokenizer(text)\n",
    "                    \n",
    "        # Include '<unk>' (unknown) and '<pad>' (padding) tokens\n",
    "        self.vocab = build_vocab_from_iterator(token_iterator(), specials=[\"<unk>\", \"<pad>\"])\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "        \n",
    "        # Store the padding index for use in batch_encode\n",
    "        self.padding_idx = self.vocab[\"<pad>\"]\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Performs initial string cleaning and lemmatization (CPU-bound).\"\"\"\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text) # Ensure the input is a string\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = [t for t in self.tokenizer(text) if t not in self.stop_words and t.isalpha()]\n",
    "        \n",
    "        if self.perform_spell_correction:\n",
    "            # Optimizing spell correction by only correcting unknown words\n",
    "            misspelled = self.spell.unknown(tokens)\n",
    "            tokens = [self.spell.correction(t) if t in misspelled else t for t in tokens]\n",
    "            \n",
    "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def encode_tensor(self, cleaned_text):\n",
    "        \"\"\"Converts text to token indices and transfers the tensor to the GPU.\"\"\"\n",
    "        tokens = self.tokenizer(cleaned_text)\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        \n",
    "        # The key step: creating the tensor and moving it to the designated device (GPU or CPU)\n",
    "        return torch.tensor(indices, dtype=torch.long).to(self.device)\n",
    "\n",
    "    def batch_encode(self):\n",
    "        \"\"\"\n",
    "        Encodes all texts and pads them to the longest sequence in the batch. \n",
    "        The final padded tensor remains on the GPU.\n",
    "        \"\"\"\n",
    "        tensors = [self.encode_tensor(t) for t in self.cleaned_texts if t]\n",
    "        \n",
    "        if not tensors: \n",
    "            return torch.tensor([], dtype=torch.long).to(self.device)\n",
    "            \n",
    "        # pad_sequence handles padding and the final output tensor is on self.device\n",
    "        return pad_sequence(tensors, batch_first=True, padding_value=self.padding_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e614306-b497-4e27-853c-2d34af7ead07",
   "metadata": {},
   "source": [
    "# Feature Engineering: Numerical and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1e34103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering completed without length errors.\n",
      "   title_len  desc_len country_code  has_salary\n",
      "0         16       905           US           1\n",
      "1         41      2077           NZ           1\n",
      "2         39       355           US           1\n",
      "3         33      2600           US           1\n",
      "4         19      1520           US           1\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Feature Engineering: Numerical and Location ---\n",
    "\n",
    "# --- CRITICAL FIX: Fill NaN/None with empty string before calling len() ---\n",
    "# This ensures that len() is only called on string objects.\n",
    "\n",
    "# 1. Fill NaNs for the text columns you need length from.\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['company_profile'] = df['company_profile'].fillna('')\n",
    "df['requirements'] = df['requirements'].fillna('')\n",
    "df['benefits'] = df['benefits'].fillna('')\n",
    "\n",
    "# 2.1 Text Length Features\n",
    "df['title_len'] = df['title'].apply(len)\n",
    "df['desc_len'] = df['description'].apply(len)\n",
    "df['profile_len'] = df['company_profile'].apply(len) \n",
    "# You should also consider requirements_len and benefits_len for richer features.\n",
    "df['req_len'] = df['requirements'].apply(len)\n",
    "df['benefits_len'] = df['benefits'].apply(len)\n",
    "\n",
    "\n",
    "# 2.2 Location Features (Splitting to Country)\n",
    "# Fill remaining NaNs in location, though it should be mostly filled\n",
    "df['location'] = df['location'].fillna('Unknown, Unknown') \n",
    "df['country_code'] = df['location'].apply(lambda x: x.split(',')[0].strip())\n",
    "\n",
    "# 2.3 Seniority Flag \n",
    "df['required_experience'] = df['required_experience'].fillna('Unknown')\n",
    "df['is_senior_role'] = df['required_experience'].str.lower().apply(\n",
    "    # Check for senior indicators\n",
    "    lambda x: 1 if 'senior' in x or 'executive' in x or 'director' in x or 'manager' in x else 0\n",
    ")\n",
    "\n",
    "# 2.4 Salary Feature (Indicator for presence)\n",
    "df['has_salary'] = df['salary_range'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "print(\"Feature Engineering completed without length errors.\")\n",
    "print(df[['title_len', 'desc_len', 'country_code', 'has_salary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f8403b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text cleaning completed in: 16.80 seconds.\n",
      "New column check: True\n",
      "Shape of Title TF-IDF features: (17880, 1500)\n",
      "Shape of Description TF-IDF features: (17880, 4000)\n",
      "Shape of Company Profile TF-IDF features: (17880, 1500)\n",
      "\n",
      "Shape of Combined Text features (Sparse): (17880, 7000)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the necessary libraries (pandas, nltk, BeautifulSoup, etc.) are imported.\n",
    "# Assuming the TextPreprocessor class has been defined in an earlier cell.\n",
    "\n",
    "# 1. Instantiate the Preprocessor (using GPUTextPreprocessor)\n",
    "preprocessor = GPUTextPreprocessor(texts=[]) # Initialize without texts first\n",
    "\n",
    "# 2. Fill NaNs in Original Text Columns (Crucial for clean_text)\n",
    "text_cols = ['title', 'description', 'company_profile', 'requirements', 'benefits']\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# 3. Apply Cleaning to Create the Missing Columns\n",
    "start_time = time.time()\n",
    "for col in text_cols:\n",
    "    df[f'cleaned_{col}'] = df[col].apply(preprocessor.clean_text)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"✅ Text cleaning completed in: {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"New column check: {'cleaned_title' in df.columns}\")\n",
    "\n",
    "# --- 4. TF-IDF VECTORIZATION (Ready to Run Now) ---\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# TF-IDF for TITLE \n",
    "tfidf_title_vectorizer = TfidfVectorizer(stop_words='english', max_features=1500, ngram_range=(1, 2))\n",
    "title_features = tfidf_title_vectorizer.fit_transform(df['cleaned_title']) # This line will now work\n",
    "print(f'Shape of Title TF-IDF features: {title_features.shape}')\n",
    "\n",
    "# TF-IDF for DESCRIPTION \n",
    "tfidf_desc_vectorizer = TfidfVectorizer(stop_words='english', max_features=4000)\n",
    "desc_features = tfidf_desc_vectorizer.fit_transform(df['cleaned_description'])\n",
    "print(f'Shape of Description TF-IDF features: {desc_features.shape}')\n",
    "\n",
    "# TF-IDF for COMPANY PROFILE \n",
    "tfidf_profile_vectorizer = TfidfVectorizer(stop_words='english', max_features=1500)\n",
    "profile_features = tfidf_profile_vectorizer.fit_transform(df['cleaned_company_profile'])\n",
    "print(f'Shape of Company Profile TF-IDF features: {profile_features.shape}')\n",
    "\n",
    "# Combine all sparse text features\n",
    "X_text_features = hstack([title_features, desc_features, profile_features])\n",
    "print(f'\\nShape of Combined Text features (Sparse): {X_text_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7a2307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Numerical/Indicator Features (Sparse): (17880, 8)\n",
      "Shape of FINAL Non-Text Feature Matrix: (17880, 1634)\n"
     ]
    }
   ],
   "source": [
    "# --- Assuming the necessary imports: from scipy.sparse import hstack, csr_matrix ---\n",
    "\n",
    "# Create the dense numerical/indicator features array (8 columns)\n",
    "X_numerical_dense = df[[\n",
    "    'telecommuting', \n",
    "    'has_company_logo', \n",
    "    'has_questions', \n",
    "    'title_len', \n",
    "    'desc_len', \n",
    "    'profile_len',\n",
    "    'is_senior_role',\n",
    "    'has_salary'\n",
    "]].values \n",
    "\n",
    "# One-Hot Encode the categorical features (Sparse Matrix)\n",
    "# X_categorical_sparse is already a sparse matrix from OneHotEncoder\n",
    "\n",
    "# --- FIX: Convert the dense NumPy array to a Sparse Matrix ---\n",
    "# Use csr_matrix for efficient horizontal stacking and final matrix structure.\n",
    "from scipy.sparse import csr_matrix\n",
    "X_numerical_sparse = csr_matrix(X_numerical_dense)\n",
    "\n",
    "print(f'Shape of Numerical/Indicator Features (Sparse): {X_numerical_sparse.shape}')\n",
    "\n",
    "# Now stack the two sparse matrices (OHE and Numerical/Indicator)\n",
    "# This forms the complete NON-TEXT feature matrix (X_non_text)\n",
    "X_non_text_features = hstack([X_categorical_sparse, X_numerical_sparse])\n",
    "\n",
    "print(f'Shape of FINAL Non-Text Feature Matrix: {X_non_text_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774dc55",
   "metadata": {},
   "source": [
    "# Final Feature Matrix and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7766d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Data Check ---\n",
      "Shape of FINAL Feature Matrix (X): (17880, 8634)\n",
      "Shape of Target Vector (y): (17880,)\n",
      "\n",
      "✅ Successfully saved X to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/X_final_features.npz\n",
      "✅ Successfully saved y to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/y_target.pkl\n",
      "✅ Successfully saved encoders to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/ml_encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- CRITICAL FIX: Import save_npz and hstack from scipy.sparse ---\n",
    "from scipy.sparse import hstack, save_npz\n",
    "\n",
    "# Assuming X_text_features, X_categorical_sparse, X_numerical_sparse, \n",
    "# df, y, and the encoder objects are defined from previous cells.\n",
    "\n",
    "# Re-define X_final (since the notebook kernel was restarted after the last run)\n",
    "# IMPORTANT: Ensure these variables are available in the current session\n",
    "# For demonstration, we assume they were successfully created previously:\n",
    "# X_final = hstack([X_text_features, X_categorical_sparse, X_numerical_sparse])\n",
    "# y = df['fraudulent'].values\n",
    "# ... (and all the tfidf_vectorizer and encoder objects) ...\n",
    "\n",
    "# --- 5. Final Feature Matrix and Saving ---\n",
    "\n",
    "# Combine ALL sparse features: Text + Categorical + Numerical\n",
    "X_final = hstack([X_text_features, X_categorical_sparse, X_numerical_sparse])\n",
    "y = df['fraudulent'].values\n",
    "\n",
    "print(\"\\n--- Final Data Check ---\")\n",
    "print(f\"Shape of FINAL Feature Matrix (X): {X_final.shape}\")\n",
    "print(f\"Shape of Target Vector (y): {y.shape}\")\n",
    "\n",
    "# --- Saving ---\n",
    "\n",
    "base_path = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/'\n",
    "save_path_X = os.path.join(base_path, 'X_final_features.npz')\n",
    "save_path_y = os.path.join(base_path, 'y_target.pkl')\n",
    "save_path_encoders = os.path.join(base_path, 'ml_encoders.pkl')\n",
    "\n",
    "# Save the final sparse feature matrix\n",
    "save_npz(save_path_X, X_final)\n",
    "\n",
    "# Save the target vector (y)\n",
    "with open(save_path_y, 'wb') as f:\n",
    "    pickle.dump(y, f)\n",
    "    \n",
    "# Save the fitted encoders/vectorizers for transforming future test data\n",
    "fitted_tools = {\n",
    "    'tfidf_title': tfidf_title_vectorizer,\n",
    "    'tfidf_desc': tfidf_desc_vectorizer,\n",
    "    'tfidf_profile': tfidf_profile_vectorizer,\n",
    "    'ohe_categorical': encoder\n",
    "}\n",
    "with open(save_path_encoders, 'wb') as f:\n",
    "    pickle.dump(fitted_tools, f)\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Successfully saved X to: {save_path_X}\")\n",
    "print(f\"✅ Successfully saved y to: {save_path_y}\")\n",
    "print(f\"✅ Successfully saved encoders to: {save_path_encoders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058421d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class GPUTextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for comprehensive text preprocessing (cleaning, tokenization,\n",
    "    lemmatization) that numericalizes the text and ensures the final tensors \n",
    "    are ready on the GPU (or CPU as fallback).\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, device='cuda', perform_spell_correction=False):\n",
    "        \n",
    "        # --- 1. Device Setup ---\n",
    "        # Explicitly check for CUDA and set the device (GPU/CPU)\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # --- 2. Preprocessing Tools ---\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.perform_spell_correction = perform_spell_correction\n",
    "        \n",
    "        if self.perform_spell_correction:\n",
    "            # Note: You need 'pip install pyspellchecker' for this to work\n",
    "            try:\n",
    "                from spellchecker import SpellChecker\n",
    "                self.spell = SpellChecker()\n",
    "            except ImportError:\n",
    "                print(\"Warning: 'spellchecker' not installed. Spell correction will be skipped.\")\n",
    "                self.perform_spell_correction = False\n",
    "        \n",
    "        # --- 3. Initial Processing (on CPU) ---\n",
    "        self.cleaned_texts = [self.clean_text(t) for t in texts]\n",
    "        \n",
    "        # --- 4. Build Vocabulary ---\n",
    "        def token_iterator():\n",
    "            for text in self.cleaned_texts:\n",
    "                # Need to check if text is not empty after cleaning\n",
    "                if text:\n",
    "                    yield self.tokenizer(text)\n",
    "                    \n",
    "        # Include '<unk>' (unknown) and '<pad>' (padding) tokens\n",
    "        self.vocab = build_vocab_from_iterator(token_iterator(), specials=[\"<unk>\", \"<pad>\"])\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "        \n",
    "        # Store the padding index for use in batch_encode\n",
    "        self.padding_idx = self.vocab[\"<pad>\"]\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Performs initial string cleaning and lemmatization (CPU-bound).\"\"\"\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text) # Ensure the input is a string\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = [t for t in self.tokenizer(text) if t not in self.stop_words and t.isalpha()]\n",
    "        \n",
    "        if self.perform_spell_correction:\n",
    "            # Optimizing spell correction by only correcting unknown words\n",
    "            misspelled = self.spell.unknown(tokens)\n",
    "            tokens = [self.spell.correction(t) if t in misspelled else t for t in tokens]\n",
    "            \n",
    "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def encode_tensor(self, cleaned_text):\n",
    "        \"\"\"Converts text to token indices and transfers the tensor to the GPU.\"\"\"\n",
    "        tokens = self.tokenizer(cleaned_text)\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        \n",
    "        # The key step: creating the tensor and moving it to the designated device (GPU or CPU)\n",
    "        return torch.tensor(indices, dtype=torch.long).to(self.device)\n",
    "\n",
    "    def batch_encode(self):\n",
    "        \"\"\"\n",
    "        Encodes all texts and pads them to the longest sequence in the batch. \n",
    "        The final padded tensor remains on the GPU.\n",
    "        \"\"\"\n",
    "        tensors = [self.encode_tensor(t) for t in self.cleaned_texts if t]\n",
    "        \n",
    "        if not tensors: \n",
    "            return torch.tensor([], dtype=torch.long).to(self.device)\n",
    "            \n",
    "        # pad_sequence handles padding and the final output tensor is on self.device\n",
    "        return pad_sequence(tensors, batch_first=True, padding_value=self.padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78363df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = (\n",
    "    df['title'].fillna('') + ' ' +\n",
    "    df['company_profile'].fillna('') + ' ' +\n",
    "    df['description'].fillna('') + ' ' +\n",
    "    df['requirements'].fillna('') + ' ' +\n",
    "    df['benefits'].fillna('')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0508be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "descriptions = df['description'].head(2000).tolist()\n",
    "# Show a few raw samples before preprocessing\n",
    "print(\"\\n--- Sample Raw Descriptions (Before Cleaning) ---\")\n",
    "for i, desc in enumerate(descriptions[:3]):\n",
    "    print(f\"[{i+1}] {desc[:300]}...\\n\")  # Truncate for readability\n",
    "\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Running optimized preprocessor (spell correction OFF)...\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['text'])\n",
    "print('Shape of TF-IDF features:', tfidf_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e66e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "preprocessor_fast = GPUTextPreprocessor(descriptions, perform_spell_correction=False)\n",
    "\n",
    "# Show cleaned versions of the same samples\n",
    "print(\"\\n--- Sample Cleaned Descriptions (After Cleaning) ---\")\n",
    "for i, cleaned in enumerate(preprocessor_fast.cleaned_texts[:3]):\n",
    "    print(f\"[{i+1}] {cleaned}\\n\")\n",
    "padded_tensor_batch = preprocessor_fast.batch_encode()\n",
    "end = time.time() \n",
    "\n",
    "print(f\"\\nTotal execution time: {end - start:.2f} seconds\")\n",
    "print(\"Tensor shape:\", padded_tensor_batch.shape)\n",
    "if padded_tensor_batch.numel() > 0:\n",
    "    print(\"First tensor sample:\", padded_tensor_batch[0][:20])\n",
    "print(\"-\" * 30)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c759f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assuming 'df' is your DataFrame after all EDA and Feature Engineering\n",
    "\n",
    "# 1. Define the new, descriptive file path\n",
    "# A good practice is to include a version number or a suffix indicating the content.\n",
    "new_file_path = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/fake_job_features_ready_for_model_v1.csv'\n",
    "\n",
    "# 2. Save the updated DataFrame to the new file\n",
    "# index=False prevents pandas from writing the DataFrame index as a new column\n",
    "try:\n",
    "    df.to_csv(new_file_path, index=False)\n",
    "    print(f\"✅ Successfully saved the feature-engineered dataset to: {new_file_path}\")\n",
    "    print(f\"Shape of the saved dataset: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving the file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
