{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb7db5d-ef11-4e96-8673-dd374774587e",
   "metadata": {},
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830e76a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "# --- Download NLTK data (if needed) ---\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba82023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.3.0+cu121\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n",
      "Is CUDA available?: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d204b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Data ---\n",
      "Loading GloVe embeddings...\n",
      "GloVe embeddings loaded for 20000 words.\n",
      "\n",
      "--- Initializing Model ---\n",
      "Using a manual positive weight for loss function: 10.00\n",
      "Model initialized on cuda.\n",
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch 01 | Training Loss: 1.0537\n",
      "Epoch 02 | Training Loss: 0.6813\n",
      "Epoch 03 | Training Loss: 0.6327\n",
      "Epoch 04 | Training Loss: 0.5481\n",
      "Epoch 05 | Training Loss: 0.5319\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Evaluating Model Performance across different thresholds ---\n",
      "\n",
      "==================== Threshold: 0.2 ====================\n",
      "Accuracy: 0.8834\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real Job       0.99      0.89      0.94      3403\n",
      "    Fake Job       0.26      0.73      0.38       173\n",
      "\n",
      "    accuracy                           0.88      3576\n",
      "   macro avg       0.62      0.81      0.66      3576\n",
      "weighted avg       0.95      0.88      0.91      3576\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "==================== Threshold: 0.3 ====================\n",
      "Accuracy: 0.9066\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real Job       0.98      0.92      0.95      3403\n",
      "    Fake Job       0.30      0.69      0.42       173\n",
      "\n",
      "    accuracy                           0.91      3576\n",
      "   macro avg       0.64      0.81      0.68      3576\n",
      "weighted avg       0.95      0.91      0.92      3576\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "==================== Threshold: 0.4 ====================\n",
      "Accuracy: 0.9237\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real Job       0.98      0.94      0.96      3403\n",
      "    Fake Job       0.34      0.64      0.45       173\n",
      "\n",
      "    accuracy                           0.92      3576\n",
      "   macro avg       0.66      0.79      0.70      3576\n",
      "weighted avg       0.95      0.92      0.93      3576\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "==================== Threshold: 0.5 ====================\n",
      "Accuracy: 0.9368\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real Job       0.98      0.96      0.97      3403\n",
      "    Fake Job       0.39      0.56      0.46       173\n",
      "\n",
      "    accuracy                           0.94      3576\n",
      "   macro avg       0.68      0.76      0.71      3576\n",
      "weighted avg       0.95      0.94      0.94      3576\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "==================== Threshold: 0.6 ====================\n",
      "Accuracy: 0.9513\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real Job       0.97      0.98      0.97      3403\n",
      "    Fake Job       0.50      0.46      0.48       173\n",
      "\n",
      "    accuracy                           0.95      3576\n",
      "   macro avg       0.73      0.72      0.73      3576\n",
      "weighted avg       0.95      0.95      0.95      3576\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "==================== Threshold: 0.7 ====================\n",
      "Accuracy: 0.9583\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Real Job       0.96      1.00      0.98      3403\n",
      "    Fake Job       0.73      0.22      0.34       173\n",
      "\n",
      "    accuracy                           0.96      3576\n",
      "   macro avg       0.85      0.61      0.66      3576\n",
      "weighted avg       0.95      0.96      0.95      3576\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "--- Saving Model and Artifacts (based on last training run) ---\n",
      "✅ Model saved to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/hybrid_model_glove_v2.pth\n",
      "✅ Vocab saved to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/vocab_glove_v2.pth\n",
      "✅ Training columns saved to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/train_cols_glove_v2.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "# --- Download NLTK data (if needed) ---\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Configuration & File Paths\n",
    "# ==============================================================================\n",
    "FILE_PATH = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/fake_job_cleaned_dataset.csv'\n",
    "GLOVE_PATH = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/glove.6B.100d.txt'\n",
    "MODEL_SAVE_PATH = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/hybrid_model_glove_v2.pth'\n",
    "VOCAB_SAVE_PATH = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/vocab_glove_v2.pth'\n",
    "COLS_SAVE_PATH = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Models/train_cols_glove_v2.pkl'\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_LEN = 512\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Reusable Functions & Classes\n",
    "# ==============================================================================\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "def create_features(df):\n",
    "    df['text'] = (\n",
    "        df['title'].fillna('') + ' ' + df['location'].fillna('') + ' ' +\n",
    "        df['description'].fillna('') + ' ' + df['requirements'].fillna('') + ' ' +\n",
    "        df['benefits'].fillna('') + ' ' + df['employment_type'].fillna('')\n",
    "    )\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    for col in ['telecommuting', 'has_company_logo', 'has_questions']:\n",
    "        df[col] = df[col].astype(float)\n",
    "    categorical_cols = ['employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
    "    return pd.get_dummies(df, columns=categorical_cols, dummy_na=True, drop_first=True)\n",
    "\n",
    "class JobDataset(Dataset):\n",
    "    def __init__(self, texts, tabular, labels, text_pipeline):\n",
    "        self.texts = texts\n",
    "        self.tabular = tabular\n",
    "        self.labels = labels\n",
    "        self.text_pipeline = text_pipeline\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        tabular_data = self.tabular[idx]\n",
    "        label = self.labels[idx]\n",
    "        processed_text = torch.tensor(self.text_pipeline(text), dtype=torch.long)\n",
    "        return processed_text, torch.tensor(tabular_data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, tabular_list, label_list = [], [], []\n",
    "    for (_text, _tabular, _label) in batch:\n",
    "        text_list.append(_text)\n",
    "        tabular_list.append(_tabular)\n",
    "        label_list.append(_label)\n",
    "    padded_texts = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    return padded_texts, torch.stack(tabular_list), torch.stack(label_list)\n",
    "    \n",
    "class HybridRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, tabular_feature_count, padding_idx, pretrained_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=False, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.tabular_fc = nn.Linear(tabular_feature_count, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        lstm_output_size = hidden_dim * 2\n",
    "        self.fc_combined = nn.Linear(lstm_output_size + 32, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, tabular_features):\n",
    "        embedded = self.embedding(text)\n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        tabular_out = self.relu(self.tabular_fc(tabular_features))\n",
    "        combined = torch.cat((hidden, tabular_out), dim=1)\n",
    "        return self.fc_combined(combined)\n",
    "\n",
    "def get_glove_embedding_matrix(vocab, glove_file_path, embedding_dim):\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for i, word in enumerate(vocab.get_itos()):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(f\"GloVe embeddings loaded for {len(embedding_matrix)} words.\")\n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Main Training & Evaluation Script\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        print(f\"Error: Dataset file '{FILE_PATH}' not found.\")\n",
    "    elif not os.path.exists(GLOVE_PATH):\n",
    "        print(f\"Error: GloVe file '{GLOVE_PATH}' not found. Please download it first.\")\n",
    "    else:\n",
    "        print(\"--- Loading and Preparing Data ---\")\n",
    "        df = pd.read_csv(FILE_PATH)\n",
    "        df_processed = create_features(df)\n",
    "        \n",
    "        y = df_processed['fraudulent'].values\n",
    "        X_text = df_processed['text']\n",
    "        \n",
    "        numeric_cols = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "        if 'fraudulent' in numeric_cols:\n",
    "            numeric_cols.remove('fraudulent')\n",
    "        X_tabular = df_processed[numeric_cols].values.astype(np.float32)\n",
    "        train_df_columns = ['text'] + numeric_cols\n",
    "\n",
    "        X_text_train, X_text_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n",
    "            X_text, X_tabular, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        tokenizer = get_tokenizer('basic_english')\n",
    "        def yield_tokens(data_iter):\n",
    "            for text in data_iter:\n",
    "                yield tokenizer(text)\n",
    "\n",
    "        vocab = build_vocab_from_iterator(yield_tokens(X_text_train), specials=[\"<unk>\", \"<pad>\"], max_tokens=VOCAB_SIZE)\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])\n",
    "        padding_idx = vocab['<pad>']\n",
    "        text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "        \n",
    "        train_dataset = JobDataset(X_text_train, X_tabular_train, pd.Series(y_train), text_pipeline)\n",
    "        test_dataset = JobDataset(X_text_test, X_tabular_test, pd.Series(y_test), text_pipeline)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "        \n",
    "        pretrained_embedding_weights = get_glove_embedding_matrix(vocab, GLOVE_PATH, EMBEDDING_DIM)\n",
    "        \n",
    "        print(\"\\n--- Initializing Model ---\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        pos_weight = torch.tensor([10.0], dtype=torch.float32).to(device)\n",
    "        print(f\"Using a manual positive weight for loss function: {pos_weight.item():.2f}\")\n",
    "        \n",
    "        model = HybridRNNModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS,\n",
    "                             BIDIRECTIONAL, DROPOUT, X_tabular.shape[1], padding_idx, pretrained_embedding_weights).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
    "        \n",
    "        print(f\"Model initialized on {device}.\")\n",
    "        print(\"\\n--- Starting Model Training ---\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for text, tab, labels in train_loader:\n",
    "                text, tab, labels = text.to(device), tab.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(text, tab).squeeze(1)\n",
    "                loss = criterion(predictions, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f'Epoch {epoch+1:02} | Training Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "        # ==============================================================================\n",
    "        # 4. UPDATED: Evaluation with Threshold Tuning\n",
    "        # ==============================================================================\n",
    "        print(\"\\n--- Evaluating Model Performance across different thresholds ---\")\n",
    "        model.eval()\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for text, tab, labels in test_loader:\n",
    "                text, tab, labels = text.to(device), tab.to(device), labels.to(device)\n",
    "                predictions = model(text, tab).squeeze(1)\n",
    "                # Get the raw probabilities by applying sigmoid\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                all_probs.extend(probabilities.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Now, test different thresholds\n",
    "        thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            print(f\"\\n==================== Threshold: {threshold} ====================\")\n",
    "            # Apply the threshold to get final predictions\n",
    "            all_preds = (all_probs >= threshold).astype(int)\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(all_labels, all_preds, target_names=['Real Job', 'Fake Job']))\n",
    "            print(\"====================================================\\n\")\n",
    "\n",
    "        # --- 5. Save the Final Model and Tokenizer ---\n",
    "        print(\"\\n--- Saving Model and Artifacts (based on last training run) ---\")\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        torch.save(vocab, VOCAB_SAVE_PATH)\n",
    "        with open(COLS_SAVE_PATH, 'wb') as f:\n",
    "            pickle.dump(train_df_columns, f)\n",
    "            \n",
    "        print(f\"✅ Model saved to: {MODEL_SAVE_PATH}\")\n",
    "        print(f\"✅ Vocab saved to: {VOCAB_SAVE_PATH}\")\n",
    "        print(f\"✅ Training columns saved to: {COLS_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30916c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
