{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a18d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from scipy.sparse import load_npz, issparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42870ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/X_final_features.npz\n",
      "Loading target from: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/y_target.pkl\n",
      "Using device for PyTorch context: cuda\n",
      "Loaded Feature Matrix Shape: (17880, 8634)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Load Final Feature Matrix and Target ---\n",
    "\n",
    "# Define the paths where the feature engineering step saved the data\n",
    "base_path = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/'\n",
    "save_path_X = os.path.join(base_path, 'X_final_features.npz')\n",
    "save_path_y = os.path.join(base_path, 'y_target.pkl')\n",
    "\n",
    "print(f\"Loading features from: {save_path_X}\")\n",
    "print(f\"Loading target from: {save_path_y}\")\n",
    "\n",
    "# Load the sparse feature matrix (X)\n",
    "X_final = load_npz(save_path_X)\n",
    "\n",
    "# Load the target vector (y)\n",
    "with open(save_path_y, 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "# Optional: Set PyTorch device context (for general GPU awareness)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device for PyTorch context: {device}\")\n",
    "print(f\"Loaded Feature Matrix Shape: {X_final.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c14a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Split Check ---\n",
      "Training set size: 12516 samples\n",
      "Testing set size: 5364 samples\n",
      "Imbalance Ratio (Non-Fraud / Fraud): 19.65\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Data Splitting and Imbalance Calculation ---\n",
    "\n",
    "# Split data into training and testing sets (70/30)\n",
    "# Stratify=y ensures both sets have the same proportion of fraudulent cases\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data Split Check ---\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Calculate the imbalance ratio for LightGBM's scale_pos_weight\n",
    "total_non_fraud = np.sum(y_train == 0)\n",
    "total_fraud = np.sum(y_train == 1)\n",
    "scale_pos_weight = total_non_fraud / total_fraud\n",
    "\n",
    "print(f\"Imbalance Ratio (Non-Fraud / Fraud): {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93496c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression (Baseline) ---\n",
      "Logistic Regression Training Complete.\n",
      "\n",
      "--- Training LightGBM Classifier ---\n",
      "LightGBM Training Complete.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "## Step 3: Model Training\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 3.1 Baseline Model: Logistic Regression\n",
    "print(\"\\n--- Training Logistic Regression (Baseline) ---\")\n",
    "lr_model = LogisticRegression(\n",
    "    solver='liblinear',        # Good for smaller datasets and L1/L2 penalties\n",
    "    class_weight='balanced',   # Automatically weighs classes inverse to their frequency\n",
    "    random_state=42,\n",
    "    max_iter=500\n",
    ")\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression Training Complete.\")\n",
    "\n",
    "# 3.2 Ensemble Model: LightGBM\n",
    "print(\"\\n--- Training LightGBM Classifier ---\")\n",
    "lgbm_model = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight, # Use the calculated ratio to handle imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "print(\"LightGBM Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f3a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "             MODEL EVALUATION (LightGBM)             \n",
      "=====================================================\n",
      "ROC AUC Score: 0.9935 (Measures class separation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9903\n",
      "\n",
      "Classification Report (Focus on Class 1: Fraudulent):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Fraud (0)       0.99      1.00      0.99      5104\n",
      "    Fraud (1)       0.96      0.83      0.89       260\n",
      "\n",
      "     accuracy                           0.99      5364\n",
      "    macro avg       0.98      0.91      0.94      5364\n",
      " weighted avg       0.99      0.99      0.99      5364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5096    8]\n",
      " [  44  216]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------\n",
    "## Step 4: Evaluation\n",
    "# ----------------------------------------------------\n",
    "\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"             MODEL EVALUATION (LightGBM)             \")\n",
    "print(\"=====================================================\")\n",
    "\n",
    "# Predict probabilities and labels on the test set\n",
    "y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# 4.1 ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f} (Measures class separation)\")\n",
    "\n",
    "# 4.2 Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 4.3 Classification Report (Precision, Recall, F1)\n",
    "print(\"\\nClassification Report (Focus on Class 1: Fraudulent):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Fraud (0)', 'Fraud (1)']))\n",
    "\n",
    "# 4.4 Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2820439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Trained Models ---\n",
      "âœ… LightGBM Model Saved to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/lgbm_model.pkl\n",
      "âœ… Logistic Regression Model Saved to: /home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/lr_model_baseline.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Define Paths (Use the same base path as before) ---\n",
    "base_path = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/'\n",
    "\n",
    "# Define specific save paths for the models\n",
    "save_path_lgbm_model = os.path.join(base_path, 'lgbm_model.pkl') \n",
    "save_path_lr_model = os.path.join(base_path, 'lr_model_baseline.pkl')\n",
    "\n",
    "print(\"\\n--- Saving Trained Models ---\")\n",
    "\n",
    "# 1. Save the LightGBM Model (Production Model)\n",
    "try:\n",
    "    with open(save_path_lgbm_model, 'wb') as f:\n",
    "        pickle.dump(lgbm_model, f)\n",
    "    print(f\"âœ… LightGBM Model Saved to: {save_path_lgbm_model}\")\n",
    "except NameError:\n",
    "    print(\"âŒ ERROR: 'lgbm_model' is not defined. Ensure the training cell was run successfully.\")\n",
    "    \n",
    "# 2. Save the Logistic Regression Model (Baseline)\n",
    "try:\n",
    "    with open(save_path_lr_model, 'wb') as f:\n",
    "        pickle.dump(lr_model, f)\n",
    "    print(f\"âœ… Logistic Regression Model Saved to: {save_path_lr_model}\")\n",
    "except NameError:\n",
    "    print(\"âŒ ERROR: 'lr_model' is not defined. Ensure the training cell was run successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b237a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All models and encoders loaded successfully.\n",
      "\n",
      "--- Enter the Raw Job Description ---\n",
      "Paste the entire text (including Qualifications/Benefits). Press ENTER twice to finish.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "          JOB CLASSIFICATION RISK REPORT ðŸŽ¯          \n",
      "=====================================================\n",
      "Job Title: Job description Ready to shape the future of work?   At Genpact, we don't just adapt to change we drive it. AI and digital innovation are redefining industries and were leading the charge. Genpacts AI Gigafactory, our industry-first accelerator, is an example of how were scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to agentic AI, our breakthrough solutions tackle companies most complex challenges.   If you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that's shaping the future, this is your moment.   Genpact (NYSE: G) is an advanced technology services and solutions company that delivers lasting value for leading enterprises globally. Through our deep business knowledge, operational excellence, and cutting-edge solutions we help companies across industries get ahead and stay ahead. Powered by curiosity, courage, and innovation, our teams implement data, technology, and AI to create tomorrow, today. Get to know us at genpact.com and on LinkedIn, X, YouTube, and Facebook   Genpact Hiring for HR Recruitment (Contractual) role, Hyderabad   Work location: Hyderabad (Only work from office) Qualifications: Any graduate is eligible Experience: 0-1yr (Previous exp. in NON-IT recruitment role) Work module: Work from office  In this role, you will be part Design and implement overall recruiting strategy and develop update job descriptions and job specifications.    Responsibilities  Perform job and task analysis to document job requirements and objectives Prepare recruitment materials and post jobs to appropriate job board/newspapers/colleges etc. Source and recruit candidates by using job boards, databases, social media etc. Screen candidates resumes and job applications Conduct interviews using various reliable recruiting and selection tools/methods to filter candidates within schedule. Onboard new employees to become fully integrated. Monitor and apply HR recruiting best practices. Provide analytical and well documented recruiting reports to the rest of the team. Act as a point of contact and build influential candidate relationships during the selection process.  Qualifications we seek in you!  Excellent communication and interpersonal skills Strong decision-making skills    Why join Genpact?  Be a transformation leader Work at the cutting edge of AI, automation, and digital innovation. Make an impact Drive chang for global enterprises and solve business challenges that matter. Accelerate your career Get hands-on experience, mentorship, and continuous learning opportunities. Work with the best Join 140,000+ bold thinkers and problem-solvers who push boundaries every day. Thrive in a values-driven culture Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress\n",
      "Company Profile Length: 2935 characters\n",
      "--------------------------------------------------\n",
      "Model Prediction: LEGITIMATE\n",
      "Confidence (P(LEGITIMATE)): 0.9999\n",
      "--------------------------------------------------\n",
      "ðŸŸ¢ High confidence real job.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/vikhil/miniconda3/envs/vikhil/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION (Paths)\n",
    "# ----------------------------\n",
    "base_path = '/home/vikhil/GROUP_1-INFOSYS/Member_Vikhil/Datasets/'\n",
    "load_path_encoders = os.path.join(base_path, 'ml_encoders.pkl')\n",
    "load_path_lgbm_model = os.path.join(base_path, 'lgbm_model.pkl')\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD MODELS AND ENCODERS\n",
    "# ----------------------------\n",
    "try:\n",
    "    with open(load_path_encoders, 'rb') as f:\n",
    "        loaded_tools = pickle.load(f)\n",
    "\n",
    "    tfidf_title_vectorizer = loaded_tools['tfidf_title']\n",
    "    tfidf_desc_vectorizer = loaded_tools['tfidf_desc']\n",
    "    tfidf_profile_vectorizer = loaded_tools['tfidf_profile']\n",
    "    ohe_categorical = loaded_tools['ohe_categorical']\n",
    "\n",
    "    with open(load_path_lgbm_model, 'rb') as f:\n",
    "        lgbm_model = pickle.load(f)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    print(\"âœ… All models and encoders loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR: Failed to load model or encoders. Check paths/files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ----------------------------\n",
    "def clean_text(text):\n",
    "    if text is None or str(text).lower() == 'nan':\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(str(text), \"html.parser\").get_text()\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokenizer(text) if t.isalpha() and t not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def segment_job_text(raw_text):\n",
    "    lines = [l.strip() for l in raw_text.split(\"\\n\") if l.strip()]\n",
    "    title = lines[0] if len(lines) > 0 else \"Unknown\"\n",
    "    company_profile = \" \".join(lines[:1])\n",
    "    description = \" \".join(lines[1:3])\n",
    "    requirements = \" \".join(lines[3:4])\n",
    "    benefits = \" \".join(lines[4:])\n",
    "    location = \"Unknown\"\n",
    "    return title, location, company_profile, description, requirements, benefits\n",
    "\n",
    "def preprocess_and_predict(job_data: dict, model):\n",
    "    df_single = pd.DataFrame([job_data])\n",
    "    \n",
    "    # --- Clean text ---\n",
    "    df_single['cleaned_title'] = df_single['title'].apply(clean_text)\n",
    "    df_single['cleaned_description'] = df_single['description'].apply(clean_text)\n",
    "    df_single['cleaned_company_profile'] = df_single['company_profile'].apply(clean_text)\n",
    "\n",
    "    # --- Derived features ---\n",
    "    df_single['title_len'] = df_single['title'].apply(len)\n",
    "    df_single['desc_len'] = df_single['description'].apply(len)\n",
    "    df_single['profile_len'] = df_single['company_profile'].apply(len)\n",
    "    df_single['is_senior_role'] = df_single['required_experience'].str.lower().apply(\n",
    "        lambda x: 1 if any(k in x for k in ['senior','executive','director','manager']) else 0\n",
    "    )\n",
    "    df_single['has_salary'] = df_single['salary_range'].apply(lambda x: 0 if x in ['Unknown',''] else 1)\n",
    "\n",
    "    # --- Text features ---\n",
    "    X_text = hstack([\n",
    "        tfidf_title_vectorizer.transform(df_single['cleaned_title']),\n",
    "        tfidf_desc_vectorizer.transform(df_single['cleaned_description']),\n",
    "        tfidf_profile_vectorizer.transform(df_single['cleaned_company_profile'])\n",
    "    ])\n",
    "\n",
    "    # --- Ensure all categorical columns exist ---\n",
    "    cat_cols = ['country_code','employment_type','required_experience','required_education',\n",
    "                'industry','function','department']\n",
    "    for col in cat_cols:\n",
    "        if col not in df_single.columns:\n",
    "            df_single[col] = 'Unknown'\n",
    "    \n",
    "    X_cat = ohe_categorical.transform(df_single[cat_cols])\n",
    "\n",
    "    # --- Numerical features used in training ONLY ---\n",
    "    num_cols = ['telecommuting','has_company_logo','has_questions','title_len','desc_len','profile_len','is_senior_role','has_salary']\n",
    "    X_num = csr_matrix(df_single[num_cols].values)\n",
    "\n",
    "    # --- Stack all features ---\n",
    "    X_final = hstack([X_text, X_cat, X_num])\n",
    "\n",
    "    # --- Prediction ---\n",
    "    pred = model.predict(X_final)[0]\n",
    "    prob = model.predict_proba(X_final)[:,1][0]  # probability of class 1 (FRAUD)\n",
    "    \n",
    "    return pred, prob\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN INTERACTIVE INPUT\n",
    "# ----------------------------\n",
    "print(\"\\n--- Enter the Raw Job Description ---\")\n",
    "print(\"Paste the entire text (including Qualifications/Benefits). Press ENTER twice to finish.\\n\")\n",
    "raw_lines = []\n",
    "while True:\n",
    "    try:\n",
    "        line = input()\n",
    "        if not line:\n",
    "            break\n",
    "        raw_lines.append(line)\n",
    "    except EOFError:\n",
    "        break\n",
    "\n",
    "raw_input_text = \"\\n\".join(raw_lines)\n",
    "title, location, company_profile, description, requirements, benefits = segment_job_text(raw_input_text)\n",
    "\n",
    "user_job_input = {\n",
    "    'job_id': 100003,\n",
    "    'title': title,\n",
    "    'location': location,\n",
    "    'department': 'Unknown',\n",
    "    'salary_range': 'Unknown',\n",
    "    'company_profile': company_profile,\n",
    "    'description': description,\n",
    "    'requirements': requirements,\n",
    "    'benefits': benefits,\n",
    "    'telecommuting': 0,\n",
    "    'has_company_logo': 0,\n",
    "    'has_questions': 0,\n",
    "    'employment_type': 'Unknown',\n",
    "    'required_experience': 'Unknown',\n",
    "    'required_education': 'Unknown',\n",
    "    'industry': 'Unknown',\n",
    "    'function': 'Unknown'\n",
    "}\n",
    "\n",
    "pred, prob = preprocess_and_predict(user_job_input, model=lgbm_model)\n",
    "status = \"FRAUDULENT\" if pred==1 else \"LEGITIMATE\"\n",
    "confidence = prob if status==\"FRAUDULENT\" else 1-prob\n",
    "\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"          JOB CLASSIFICATION RISK REPORT ðŸŽ¯          \")\n",
    "print(\"=====================================================\")\n",
    "print(f\"Job Title: {user_job_input['title']}\")\n",
    "print(f\"Company Profile Length: {len(user_job_input['company_profile'])} characters\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Model Prediction: {status}\")\n",
    "print(f\"Confidence (P({status})): {confidence:.4f}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# --- Application advice ---\n",
    "if status==\"LEGITIMATE\":\n",
    "    print(\"ðŸŸ¢ High confidence real job.\" if confidence>0.95 else \"ðŸŸ¡ Low-risk real job. Verify details.\")\n",
    "else:\n",
    "    print(\"ðŸ”´ HIGH-RISK FRAUD ALERT.\" if prob>0.90 else \"ðŸŸ  Medium-risk fraud. Manual review recommended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386362b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
